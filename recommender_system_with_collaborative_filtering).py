# -*- coding: utf-8 -*-
"""Recommender System with Collaborative Filtering)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nauvidscience/recommender-system-with-collaborative-filtering.f34d1927-4fc6-4008-a9a1-a41735a4bbdb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251215/auto/storage/goog4_request%26X-Goog-Date%3D20251215T021035Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D34a26b1684d2dfc70ebeaa6732b7ea2031aa7d068a095d788ba6cdf1c17e6a415fe83a1016fcf6ca33a59d9adad7384d4da12894cc0f9566edaa32ccd536c73bebf255a9c590fdf5f64ec702bc69ebe3663ff1bbd9c0e15ce4131c226318a935cf68210e31e7603c2a0602ab7181fb1327e45090c0730dc6d13e4e83e1af0e09026cf2f4913ccb0b7ae086025755a656d27d6e3e93207e55466ee4b32f9a70863b46a895165b9cf33754ebdc129de3029520c0770565391bbfa0ccb387c91fa20202a3e6e03241e1ad793dd2850d2cf31800b549d94f1258020d659ebd637fb7bcaac4d9d54db2909025d79f02d5c1eaafb93ebf6ac24c96a34f5307862e5121
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
grouplens_movielens_latest_small_path = kagglehub.dataset_download('grouplens/movielens-latest-small')

print('Data source import complete.')

"""# Movie Recommendation System

## Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, List

import re, warnings, random, joblib
from wordcloud import WordCloud
from collections import Counter

import scipy
from scipy import sparse
from scipy.sparse import csr_matrix, hstack

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.metrics import MeanSquaredError

warnings.filterwarnings("ignore")

"""## Configurations & Load Dataset"""

# Cfg
BASE_PATH = "/kaggle/input/movielens-latest-small"
SEED = 2025
np.random.seed(SEED)

warnings.filterwarnings("ignore")
pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 100)

# Load Data
ratings = pd.read_csv(f"{BASE_PATH}/ratings.csv")
movies = pd.read_csv(f"{BASE_PATH}/movies.csv")

"""## Data Understanding & EDA

### Data Understanding
"""

# What dataset looks
display(ratings.head(), movies.head())

display(ratings.info(), movies.info())

"""#### **Ratings Data File Structure (ratings.csv)**
All ratings are contained in the file ratings.csv. Each line of this file after the header row represents one rating of one movie by one user, and has the following format:

> **userId,movieId,rating,timestamp**

The lines within this file are ordered first by userId, then, within user, by movieId.

Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).

Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.

---

#### **Movies Data File Structure (movies.csv)**
Movie information is contained in the file movies.csv. Each line of this file after the header row represents one movie, and has the following format:

> **movieId,title,genres**

Movie titles are entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.

---
- **User Ids**
MovieLens users were selected at random for inclusion. Their ids have been anonymized. User ids are consistent between ratings.csv and tags.csv (i.e., the same id refers to the same user across the two files).

- **Movie Ids**
Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (e.g., id 1 corresponds to the URL https://movielens.org/movies/1). Movie ids are consistent between ratings.csv, tags.csv, movies.csv, and links.csv (i.e., the same id refers to the same movie across these four data files).

- **Timestamps**
represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.
"""

def print_basic_info() -> None:
    print("Ratings shape :", ratings.shape)
    print("Movies shape  :", movies.shape)

    print("\nNumber of unique userID by ratings :", ratings['userId'].nunique())
    print("Number of unique movies by ratings  :", ratings['movieId'].nunique())
    print("Number of unique movies by movies  :", movies['movieId'].nunique())
    print("Number of unique title by movies  :", movies['title'].nunique())

print_basic_info()

"""#### information obtained
Masing masing dataset memiliki ukuran yang berbeda-beda yang berarti harus melakukan matching untuk membuat dataset baru untuk tahapan selanjutnya.
Terdapat mismatch antara unique movies dan title yang mana ada 5 film yang disimpan dalam dataset dengan 2 Id. Proses ini akan dilakukan pada tahap Preprocessing

### EDA

#### Sparsity Analysis
"""

# Sparsity
n_users = ratings['userId'].nunique()
n_items = ratings['movieId'].nunique()
n_interactions = len(ratings)

sparsity = 1 - (n_interactions / (n_users * n_items))
print(f"Sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)")

"""Nilai sparsity sebesar 98.30% menunjukkan bahwa matriks user–item sangat jarang terisi. Kondisi ini umum dijumpai pada sistem rekomendasi dunia nyata dan menjadi tantangan utama dalam collaborative filtering. Oleh karena itu, digunakan pendekatan Neural Collaborative Filtering yang mampu mempelajari representasi laten pengguna dan item meskipun data bersifat sparse."""

# Rating Distribution
def rating_displot(_df: pd.DataFrame) -> None:
    df = _df.copy()

    plt.figure(figsize=(7, 4))
    sns.countplot(x='rating', data=df)
    plt.title("Rating Distribution")
    plt.show()

# Call ratings
rating_displot(ratings)

"""Bentuk distribusi rating yang dihasilkan memiliki bentuk skewed left yang berarti mayoritas orang melakukan rating terhadap film yang disukai, karena pada case ini rating >= 3.0 dianggap bahwa user menyukai film tersebut"""

# User Activity Distribution
user_activity = ratings.groupby("userId")['rating'].count()

plt.figure(figsize=(7,4))
sns.histplot(user_activity, bins=40)
plt.title("Distribution of Ratings per User")
plt.show()

print(user_activity.describe())

# Movie Populairty Distribution
item_popularity = ratings.groupby("movieId")['rating'].count()

plt.figure(figsize=(7,4))
sns.histplot(item_popularity, bins=40)
plt.title("Distribution of Ratings per Movie")
plt.show()

print(item_popularity.describe())

item_popularity_sorted = item_popularity.sort_values(ascending=False).reset_index(drop=True)

plt.figure(figsize=(10,4))
plt.plot(item_popularity_sorted)
plt.title("Long-Tail: Movie Popularity Curve")
plt.xlabel("Movies")
plt.ylabel("Rating Count")
plt.show()

# Title Parsing → Extract Release Year
def extract_year(title):
    match = re.search(r'\((\d{4})\)', title)
    return int(match.group(1)) if match else np.nan

year = movies['title'].apply(extract_year)

plt.figure(figsize=(10,4))
sns.histplot(year, bins=40)
plt.title("Distribution of Movie Release Year")
plt.show()

year.describe()

# Cold-Start Analysis
cold_items = (item_popularity <= 5).sum()
cold_percentage = cold_items / n_items * 100

print("Cold-start items :", cold_items)
print(f"Percentage        : {cold_percentage:.2f}%")

cold_users = (user_activity <= 5).sum()
cold_users_percentage = cold_users / n_users * 100

print("Cold-start users :", cold_users)
print(f"Percentage        : {cold_users_percentage:.2f}%")

"""Analisis cold-start menunjukkan bahwa dataset tidak memiliki permasalahan cold-start user, namun memiliki proporsi cold-start item yang tinggi.

## Preprocessing
"""

display(ratings.head(), movies.head())

"""### Fixing the Mismathc between 'movieId' and 'titles'"""

movies.title.value_counts().sort_values(ascending=False).head(6)

"""#### information obtained

Terlihat jelas bahawa terdapat 5 title yang disimpan dengan Id yang berbeda
"""

# Cek Films
filtered = movies[movies['title'].isin([
    'Emma (1996)',
    'Confessions of a Dangerous Mind (2002)',
    'Eros (2004)',
    'Saturn 3 (1980)',
    'War of the Worlds (2005)'
])].sort_values("title")

filtered

# Mengambil value movieId
movie_ids = filtered['movieId'].values.tolist()
movie_ids

# hitung jumlah rating per movieId
counts = ratings[ratings['movieId'].isin(movie_ids)]['movieId'].value_counts()

# tambahkan ke filtered
filtered['rating_count'] = filtered['movieId'].map(counts)
filtered

# Fixing
def matching_movieId(_df: pd.DataFrame) -> pd.DataFrame:
    df = _df.copy()

    # Daftar ID yang ingin dihapus
    remove_ids = [144606, 26958, 147002, 168358, 64997]

    # deleting the id who is less watched
    df = df[~df['movieId'].isin(remove_ids)]
    return df

# Call
movies = matching_movieId(movies)

print("Number of unique movies by movies  :", movies['movieId'].nunique())
print("Number of unique title by movies  :", movies['title'].nunique())

"""### Menggabungkan dataset Movies dan Ratings"""

movies_ratings = pd.merge(movies, ratings, on='movieId')
movies_ratings.head()

# dropping 'timestamp' column
movies_ratings = movies_ratings[['userId','movieId', 'title', 'genres', 'rating']]

# Mengurutkan berdasarkan UserId kemudian movieId
movies_ratings.sort_values(['userId','movieId'], inplace=True)

# resetting the index
movies_ratings.reset_index(drop=True, inplace=True)

movies_ratings.head()

# Cek unique
print(f"userId\t: {movies_ratings.userId.nunique()}")
print(f"movieId\t: {movies_ratings.movieId.nunique()}")
print(f"title\t: {movies_ratings.title.nunique()}")

"""### Normalisasi Text"""

# Normalisasi text
def normalize(_df: pd.DataFrame) -> pd.DataFrame:
    df = _df.copy()

    # Normalisasi title & genres
    df["title"] = df["title"].str.strip()
    df["genres"] = df["genres"].str.strip()
    return df

movies_ratings = normalize(movies_ratings)

"""### Memproses Tahun Pada Fitur Title"""

# Mengekstrak
movies_ratings['year'] = movies_ratings['title'].str[-5:-1]
movies_ratings['year'].unique()

# Cek year yang tidak valid
year_error = ['irro','atso',' Bab', 'ron ','r On', 'lon ','imal', 'osmo', 'he O', ' Roa', 'ligh', 'erso']
filtered = movies_ratings[movies_ratings["year"].isin(year_error)][["title", "year"]].drop_duplicates(subset=["year"])

filtered

"""Berdasarkan pencarian di internet didapatkan bahwa:

---

- Black Mirror rilis tahun 2018
- The Adventures of Sherlock Holmes and Doctor Watson rilis tahun 1980
- Maria Bamford: Old Baby rilis tahun 2017
- Generation Iron 2 rilis tahun 2017
- Ready Player One rilis tahun 2018
- Babylon 5	rilis tahun 1993
- Nocturnal Animals	rilis tahun 2016
- Cosmos rilis tahun 2014
- The OA rilis tahun 2016
- Hyena Road rilis tahun 2015
- Moonlight rilis tahun 2016
- Paterson rilis tahun 2016
"""

# Mapping
movies_ratings['year'] = movies_ratings['year'].replace('irro',2018)
movies_ratings['year'] = movies_ratings['year'].replace('atso',1980)
movies_ratings['year'] = movies_ratings['year'].replace(' Bab',2017)
movies_ratings['year'] = movies_ratings['year'].replace('ron ',2017)
movies_ratings['year'] = movies_ratings['year'].replace('r On',2018)
movies_ratings['year'] = movies_ratings['year'].replace('lon ',1993)
movies_ratings['year'] = movies_ratings['year'].replace('imal',2016)
movies_ratings['year'] = movies_ratings['year'].replace('osmo',2014)
movies_ratings['year'] = movies_ratings['year'].replace('he O',2016)
movies_ratings['year'] = movies_ratings['year'].replace(' Roa',2015)
movies_ratings['year'] = movies_ratings['year'].replace('ligh',2016)
movies_ratings['year'] = movies_ratings['year'].replace('erso',2016)

# Retype
movies_ratings["year"] = movies_ratings["year"].astype(int)

movies_ratings.year.unique()

"""### Memproses Fitur Genres"""

# Cek unique
movies_ratings["genres"].unique()

""" Terdapat anomlay pada value fitur genres, yaitu '(no genres listed)'."""

# Periksa movieId dengan genres '(no genres listed)'
movies_ratings[movies_ratings['genres']=='(no genres listed)'].drop_duplicates(subset='movieId')[['movieId', 'title', 'genres']]

"""Beberapa judul tidak memiliki informasi genres secara eksplisit, maka akan melakukan pencarian di internet"""

# Replacing & Mapping
movies_ratings.loc[movies_ratings['movieId']==122896,"genres"] = 'Adventure|Action|Fantasy'
movies_ratings.loc[movies_ratings['movieId']==114335,"genres"] = 'Fantasy'
movies_ratings.loc[movies_ratings['movieId']==174403,"genres"] = 'Documentary|Biography'
movies_ratings.loc[movies_ratings['movieId']==172591,"genres"] = 'Crime|Drama|Thriller'
movies_ratings.loc[movies_ratings['movieId']==176601,"genres"] = 'Sci-Fi|Fantasy'
movies_ratings.loc[movies_ratings['movieId']==155589,"genres"] = 'Comedy'
movies_ratings.loc[movies_ratings['movieId']==147250,"genres"] = 'Crime|Mystery|Romance'
movies_ratings.loc[movies_ratings['movieId']==171749,"genres"] = 'Animation|Crime|Drama'
movies_ratings.loc[movies_ratings['movieId']==173535,"genres"] = 'Crime|Drama|Mystery'
movies_ratings.loc[movies_ratings['movieId']==134861,"genres"] = 'Comedy'
movies_ratings.loc[movies_ratings['movieId']==159161,"genres"] = 'Comedy'
movies_ratings.loc[movies_ratings['movieId']==171631,"genres"] = 'Documentary|Comedy'
movies_ratings.loc[movies_ratings['movieId']==171891,"genres"] = 'Documentary'
movies_ratings.loc[movies_ratings['movieId']==142456,"genres"] = 'Comedy|Fantasy'
movies_ratings.loc[movies_ratings['movieId']==181413,"genres"] = 'Documentary'
movies_ratings.loc[movies_ratings['movieId']==159779,"genres"] = 'Comedy|Fantasy'
movies_ratings.loc[movies_ratings['movieId']==169034,"genres"] = 'Music'
movies_ratings.loc[movies_ratings['movieId']==171495,"genres"] = 'Sci-Fi'
movies_ratings.loc[movies_ratings['movieId']==172497,"genres"] = 'Action|Sci-Fi'
movies_ratings.loc[movies_ratings['movieId']==166024,"genres"] = 'Drama|Music'
movies_ratings.loc[movies_ratings['movieId']==167570,"genres"] = 'Drama|Fantasy|Mystery'
movies_ratings.loc[movies_ratings['movieId']==129250,"genres"] = 'Comedy'
movies_ratings.loc[movies_ratings['movieId']==143410,"genres"] = 'Action|Drama|War'
movies_ratings.loc[movies_ratings['movieId']==149330,"genres"] = 'Animation|Sci-Fi'
movies_ratings.loc[movies_ratings['movieId']==182727,"genres"] = 'Music'
movies_ratings.loc[movies_ratings['movieId']==152037,"genres"] = 'Romance|Music'
movies_ratings.loc[movies_ratings['movieId']==165489,"genres"] = 'Drama|Animation|History'
movies_ratings.loc[movies_ratings['movieId']==141866,"genres"] = 'Horror|Music|Thriller'
movies_ratings.loc[movies_ratings['movieId']==122888,"genres"] = 'Action|Adventure|Drama'
movies_ratings.loc[movies_ratings['movieId']==156605,"genres"] = 'Comedy|Drama|Romance'
movies_ratings.loc[movies_ratings['movieId']==141131,"genres"] = 'Action|Mystery|Sci-Fi'
movies_ratings.loc[movies_ratings['movieId']==181719,"genres"] = 'Biography|Drama'
movies_ratings.loc[movies_ratings['movieId']==132084,"genres"] = 'Drama|Romance'
movies_ratings.loc[movies_ratings['movieId']==161008,"genres"] = 'Drama|Music|Romance'

# replacing 'musical' with 'music' as both have same meaning
movies_ratings['genres'] = movies_ratings['genres'].str.replace('Musical','Music')

# One-hot encoding genre
genre_ohe = movies_ratings['genres'].str.get_dummies(sep='|')

# Gabungkan kembali ke dataframe
movies_ratings = pd.concat([movies_ratings, genre_ohe], axis=1)

movies_ratings.head()

movies_ratings.info()

"""## Collaborative Filtering"""

df = movies_ratings.copy(deep=True)
df.head()

# Memproses fitur userId
user_ids = df['userId'].unique().tolist()

# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Memproses fitur movieId
movie_ids = df['movieId'].unique().tolist()

# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

# Mapping userID ke dataframe user
df['user'] = df['userId'].map(user_to_user_encoded)

# Mapping placeID ke dataframe resto
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(f"total user: {num_users}")

# Mendapatkan jumlah movie
num_movies = len(movie_encoded_to_movie)
print(f"total movie: {num_movies}")

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movies, min_rating, max_rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=SEED)
df

# Membuat variabel untuk matching antara data user dengan movie menjadi satu
X = df[['user', 'movie']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].values.astype(np.float32)

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
X_train, X_val, y_train, y_val = (
    X[:train_indices],
    X[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(f"Ukuran data latih: {len(X_train)}")
print(f"Ukuran data validasi: {len(X_val)}")

X_train_user = X_train[:, 0].astype("int32")
X_train_item = X_train[:, 1].astype("int32")

X_val_user = X_val[:, 0].astype("int32")
X_val_item = X_val[:, 1].astype("int32")

y_train = y_train.astype("float32")
y_val = y_val.astype("float32")

# Preparation data train & validasi
BATCH_SIZE = 256
AUTOTUNE = tf.data.AUTOTUNE
# sample_weight = np.where(y_train >= 3.0, 2.0, 1.0)

# To tensor
train_ds = tf.data.Dataset.from_tensor_slices(
    (
        {"user_id": X_train_user,
         "item_id": X_train_item},
        y_train

    )
)


val_ds = tf.data.Dataset.from_tensor_slices(
    (
        {
            "user_id": X_val_user,
            "item_id": X_val_item
        },
        y_val
    )
)

train_ds = (
    train_ds
    .shuffle(100_000)
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

val_ds = (
    val_ds
    .batch(BATCH_SIZE)
    .prefetch(tf.data.AUTOTUNE)
)

"""### Model NFC"""

# Membuat class collaborative filtering
class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super().__init__(**kwargs)

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-5),
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.movie_embedding = layers.Embedding(
            num_movies,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-5),
        )
        self.movie_bias = layers.Embedding(num_movies, 1)

    def call(self, inputs):
        user_id = inputs["user_id"]
        movie_id = inputs["item_id"]

        user_vector = self.user_embedding(user_id)
        movie_vector = self.movie_embedding(movie_id)

        user_bias = self.user_bias(user_id)
        movie_bias = self.movie_bias(movie_id)

        # Dot product
        dot = tf.reduce_sum(user_vector * movie_vector, axis=1, keepdims=True)

        # Linear output (REGRESSION)
        x = dot + user_bias + movie_bias
        return x

# Inisiasi model
model = RecommenderNet(num_users, num_movies, 32)

# Compile
model.compile(
    loss=tf.keras.losses.MeanSquaredError(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Membuat callbacks
def callbacks_config():
    # Early Stopping
    early_stop = tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=3,
        restore_best_weights=True,
        verbose=1
    )

    # Reduce learning rate
    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.5,
        patience=2,
        min_lr=1e-5,
        verbose=1
    )

    # Checkpoint
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        "best_model.keras",
        monitor="val_loss",
        save_best_only=True,
        verbose=1
    )

    return [early_stop, reduce_lr, checkpoint]

callbacks = callbacks_config()

# Train model
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=100,
    callbacks=callbacks
)

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""## Evaluation"""

# Rebuild dataframe ground truth
df_train = pd.DataFrame({
    "user": X_train[:, 0],
    "movie": X_train[:, 1],
    "rating": y_train
})

df_val = pd.DataFrame({
    "user": X_val[:, 0],
    "movie": X_val[:, 1],
    "rating": y_val
})

# Menambahkan userId dan movieId asli untuk evaluasi
df_train["userId"] = df_train["user"].map(user_encoded_to_user)
df_train["movieId"] = df_train["movie"].map(movie_encoded_to_movie)

df_val["userId"] = df_val["user"].map(user_encoded_to_user)
df_val["movieId"] = df_val["movie"].map(movie_encoded_to_movie)

# Fungsi rekomendasi top-k
def recommend_top_k(model, user_id, user_to_user_encoded, movie_to_movie_encoded, movie_encoded_to_movie, df_train, k=10):
    # Skip user yang tidak dikenal
    if user_id not in user_to_user_encoded:
        return []

    user_enc = user_to_user_encoded[user_id]

    # Movie yang sudah ditonton user (TRAIN)
    watched = df_train[df_train["userId"] == user_id]["movieId"].values
    watched_enc = [
        movie_to_movie_encoded[m]
        for m in watched
        if m in movie_to_movie_encoded
    ]

    # Semua movie
    all_movies = np.arange(len(movie_encoded_to_movie))

    # Kandidat = belum ditonton
    candidates = np.setdiff1d(all_movies, watched_enc)
    if len(candidates) == 0:
        return []

    user_array = np.full(len(candidates), user_enc)

    # Prediksi rating
    preds = model.predict(
        {
            "user_id": user_array,
            "item_id": candidates
        },
        verbose=0
    ).reshape(-1)

    # Top-K
    top_k_idx = preds.argsort()[-k:][::-1]
    recommended = [
        movie_encoded_to_movie[candidates[i]]
        for i in top_k_idx
    ]

    return recommended

def evaluate_top_k(model, df_train, df_val, user_to_user_encoded, movie_to_movie_encoded, movie_encoded_to_movie, k=10, rating_threshold=4.0):
    precisions = []
    recalls = []
    hit_rates = []
    ndcgs = []

    users = df_val["userId"].unique()

    for user_id in users:
        # Ground truth (relevant items)
        relevant_items = df_val[
            (df_val["userId"] == user_id) &
            (df_val["rating"] >= rating_threshold)
        ]["movieId"].values

        if len(relevant_items) == 0:
            continue

        # Rekomendasi
        recommended_items = recommend_top_k(
            model,
            user_id,
            user_to_user_encoded,
            movie_to_movie_encoded,
            movie_encoded_to_movie,
            df_train,
            k
        )

        if len(recommended_items) == 0:
            continue

        relevant_set = set(relevant_items)
        recommended_set = set(recommended_items)

        # Hit
        hits = relevant_set & recommended_set
        num_hits = len(hits)

        # Precision@K
        precisions.append(num_hits / k)

        # Recall@K
        recalls.append(num_hits / len(relevant_set))

        # Hit Rate@K (minimal 1 hit)
        hit_rates.append(1.0 if num_hits > 0 else 0.0)

        # NDCG@K
        dcg = 0.0
        for idx, item in enumerate(recommended_items):
            if item in relevant_set:
                dcg += 1.0 / np.log2(idx + 2)

        ideal_dcg = sum(
            1.0 / np.log2(i + 2)
            for i in range(min(len(relevant_set), k))
        )

        ndcgs.append(dcg / ideal_dcg if ideal_dcg > 0 else 0.0)

    return {
        "Precision@K": np.mean(precisions),
        "Recall@K": np.mean(recalls),
        "HitRate@K": np.mean(hit_rates),
        "NDCG@K": np.mean(ndcgs)
    }

# Evaluasi
metrics = evaluate_top_k(
    model=model,
    df_train=df_train,
    df_val=df_val,
    user_to_user_encoded=user_to_user_encoded,
    movie_to_movie_encoded=movie_to_movie_encoded,
    movie_encoded_to_movie=movie_encoded_to_movie,
    k=10,
    rating_threshold=3.0
)

for k, v in metrics.items():
    print(f"{k}: {v:.4f}")

"""Hasil evaluasi menunjukkan bahwa model Neural Collaborative Filtering mampu memberikan setidaknya satu rekomendasi relevan kepada sekitar 34.65% pengguna (HitRate@10). Namun, nilai Precision@10 dan Recall@10 relatif rendah, yang mencerminkan tingginya tingkat sparsity dan proporsi cold-start item dalam dataset. Selain itu, nilai NDCG@10 yang rendah mengindikasikan bahwa item relevan belum secara konsisten muncul pada posisi teratas rekomendasi. Kondisi ini sejalan dengan karakteristik collaborative filtering berbasis rating prediction.


Dengan sparsity tinggi dan dominasi cold-start item, hasil evaluasi NFC menunjukkan performa yang realistis, stabil, dan konsisten dengan karakteristik data, khususnya dalam memberikan setidaknya satu rekomendasi relevan kepada pengguna.

### Inference
"""

# Ambil embedding dan bias dari model
user_emb = model.user_embedding.get_weights()[0]
item_emb = model.movie_embedding.get_weights()[0]

user_bias = model.user_bias.get_weights()[0].reshape(-1)
item_bias = model.movie_bias.get_weights()[0].reshape(-1)


# Sample
user_id = 203

def recommend_movies(
    user_id,
    df,
    df_train,
    user_to_user_encoded,
    movie_to_movie_encoded,
    movie_encoded_to_movie,
    user_emb,
    item_emb,
    user_bias,
    item_bias,
    k=10
):
    # User tidak dikenal
    if user_id not in user_to_user_encoded:
        raise ValueError(f"User {user_id} tidak ditemukan")

    u = user_to_user_encoded[user_id]

    # Movie yang sudah ditonton user (train)
    watched = df_train[df_train["userId"] == user_id]["movieId"].values
    watched_enc = {
        movie_to_movie_encoded[m]
        for m in watched
        if m in movie_to_movie_encoded
    }

    # Semua movie
    all_items = np.arange(item_emb.shape[0])

    # Kandidat = belum ditonton
    candidates = np.array(
        [i for i in all_items if i not in watched_enc]
    )

    if len(candidates) == 0:
        return []

    # Skor prediksi (vectorized)
    scores = (
        item_emb[candidates] @ user_emb[u]
        + item_bias[candidates]
        + user_bias[u]
    )

    # Ambil Top-K
    top_k_idx = np.argpartition(scores, -k)[-k:]
    top_k_idx = top_k_idx[np.argsort(scores[top_k_idx])[::-1]]

    # Decode movieId
    recommended_movie_ids = [
        movie_encoded_to_movie[candidates[i]]
        for i in top_k_idx
    ]

    # Ambil info film
    rec_df = (
        df[df["movieId"].isin(recommended_movie_ids)]
        .drop_duplicates("movieId")
        .set_index("movieId")
        .loc[recommended_movie_ids]
        .reset_index()
    )

    rec_df["pred_score"] = scores[top_k_idx]

    return rec_df[["movieId", "title", "pred_score"]]

def show_user_history(user_id, df, top_n=10):
    return (
        df[df["userId"] == user_id]
        .sort_values("rating", ascending=False)
        .head(top_n)[["movieId", "title", "rating"]]
    )

print(f"Film yang sudah ditonton dan disukai oleh user_id {user_id}")
show_user_history(user_id, df)

user_id = df["userId"].iloc[0]  # contoh user

recommendations = recommend_movies(
    user_id=user_id,
    df=df,
    df_train=df_train,
    user_to_user_encoded=user_to_user_encoded,
    movie_to_movie_encoded=movie_to_movie_encoded,
    movie_encoded_to_movie=movie_encoded_to_movie,
    user_emb=user_emb,
    item_emb=item_emb,
    user_bias=user_bias,
    item_bias=item_bias,
    k=10
)


print(f"Film hasil sistem rekomendasi: ")
recommendations

"""## KESIMPULAN"""

Precision@K: 0.0482
Recall@K: 0.0408
HitRate@K: 0.3465
NDCG@K: 0.0563

"""Berdasarkan seluruh tahapan yang telah dilakukan, mulai dari eksplorasi data, pembangunan model, hingga evaluasi performa, dapat disimpulkan bahwa sistem rekomendasi film yang dibangun menggunakan pendekatan Neural Collaborative Filtering (NFC) mampu memberikan rekomendasi yang relevan secara terbatas namun konsisten dengan karakteristik data yang digunakan.

Hasil analisis eksploratif menunjukkan bahwa dataset memiliki tingkat sparsity yang sangat tinggi, yaitu sebesar 98.30%, yang mengindikasikan bahwa sebagian besar pasangan user–item tidak memiliki interaksi. Selain itu, analisis cold-start memperlihatkan bahwa 66.39% item tergolong cold-start, sementara tidak ditemukan cold-start user. Kondisi ini mencerminkan fenomena long-tail distribution yang umum pada sistem rekomendasi dunia nyata, di mana hanya sebagian kecil item yang sangat populer dan sebagian besar item jarang mendapatkan interaksi.

Dalam kondisi data yang sangat sparse tersebut, penggunaan Neural Collaborative Filtering menjadi pilihan yang tepat karena model ini mampu mempelajari representasi laten pengguna dan item melalui embedding, sehingga dapat melakukan generalisasi meskipun jumlah interaksi terbatas. Model NFC yang dibangun memanfaatkan embedding user dan item serta bias untuk memprediksi skor preferensi, yang kemudian digunakan untuk melakukan perankingan item pada tahap rekomendasi.

Hasil evaluasi menunjukkan bahwa model mencapai nilai Precision@10 sebesar 0.0482, Recall@10 sebesar 0.0408, HitRate@10 sebesar 0.3465, dan NDCG@10 sebesar 0.0563. Nilai Precision@10 dan Recall@10 yang relatif rendah menunjukkan bahwa hanya sebagian kecil item relevan yang berhasil direkomendasikan dalam Top-10, yang merupakan konsekuensi langsung dari tingginya sparsity dan dominasi item cold-start. Namun demikian, nilai HitRate@10 yang mencapai sekitar 34.65% menunjukkan bahwa model mampu memberikan setidaknya satu rekomendasi relevan kepada sekitar sepertiga pengguna, yang merupakan indikator penting dari sudut pandang pengalaman pengguna (user experience).

Nilai NDCG@10 yang masih rendah mengindikasikan bahwa urutan item relevan dalam daftar rekomendasi belum optimal, yang dapat dijelaskan oleh fakta bahwa proses pelatihan model masih berfokus pada prediksi rating (rating prediction) dan belum secara eksplisit mengoptimalkan fungsi ranking. Meskipun demikian, hasil evaluasi secara keseluruhan berada dalam rentang yang wajar dan realistis untuk sistem rekomendasi berbasis collaborative filtering pada data dengan tingkat sparsity tinggi.

Secara keseluruhan, sistem rekomendasi yang dibangun telah berhasil mengimplementasikan pipeline end-to-end, mulai dari pemrosesan data, pelatihan model Neural Collaborative Filtering, evaluasi berbasis metrik ranking, hingga tahap inference untuk menghasilkan rekomendasi film yang dapat diinterpretasikan. Keterbatasan utama sistem ini terletak pada ketidakmampuannya menangani item cold-start secara optimal dan belum maksimalnya kualitas perankingan. Oleh karena itu, pengembangan lebih lanjut dapat dilakukan dengan mengintegrasikan pendekatan hybrid berbasis konten, penggunaan ranking-aware loss function, atau penanganan cold-start secara eksplisit untuk meningkatkan kualitas rekomendasi di masa mendatang.
"""